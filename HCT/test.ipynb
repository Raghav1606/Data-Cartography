{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /home/v-biyangguo/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9274b98cf63e4e20b154556afc1bcf91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234e8f2b7e824737b11fc80a2aa51ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928425bf8e7b4abcadfd6258c8603dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1464f6e488444e6688725d758884c3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/v-biyangguo/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff91ac1313ad461eb0e1f9ec9fa217c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('glue','sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970ec2e0be3f4ec591354e4ee2a95c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c8c1ecd10f48a69030fff9508c625e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa93d7649294a4f8c18a859f5071758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fb84b91c944182b7ece80094352bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb13645ce6eb448097216d9356d2da80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec13f4cd46e34bf7a436994b7b455fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea320e42234e438fae5be3918a8186c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    texts = (examples['sentence'], None)\n",
    "    result = tokenizer(*texts, padding=\"max_length\", max_length=100, truncation=True)\n",
    "    if \"label\" in examples:\n",
    "        result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,  \n",
    "    desc=\"Running tokenizer on dataset\",)\n",
    "processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels']),\n",
       " torch.Size([8, 100]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "dataloader = DataLoader(processed_datasets['validation'], shuffle=True, collate_fn=data_collator, batch_size=8)\n",
    "c = 0\n",
    "for batch in dataloader:\n",
    "    pass\n",
    "batch.keys(), batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1005d24c983d4455aae050c7c3819d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('bert-base-cased', num_labels=2)\n",
    "encoder = AutoModel.from_pretrained('bert-base-cased')\n",
    "classifier_dropout = (config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob)\n",
    "dropout = nn.Dropout(classifier_dropout)\n",
    "classifier_easy = nn.Linear(config.hidden_size, config.num_labels)\n",
    "classifier_hard = nn.Linear(config.hidden_size, config.num_labels)\n",
    "# 2 experts: easy or hard\n",
    "hardness_gate = nn.Linear(config.hidden_size,2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_wo_labels = {k:v for k,v in batch.items() if k != 'labels'}\n",
    "outputs = encoder(**batch_wo_labels)\n",
    "pooled_output = outputs[1]\n",
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1883, -0.2964],\n",
       "        [ 0.1280, -0.3338],\n",
       "        [ 0.1373, -0.3579],\n",
       "        [ 0.2173, -0.2385],\n",
       "        [ 0.0645, -0.3355],\n",
       "        [ 0.1644, -0.2774],\n",
       "        [ 0.1745, -0.3913],\n",
       "        [ 0.2355, -0.2404]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "logits = model(**batch).logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4759e-09, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch import nn\n",
    "# kld_loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "kld_loss_fct(\n",
    "            nn.functional.log_softmax(logits / 1, dim=-1),\n",
    "            nn.functional.softmax(logits / 1, dim=-1),\n",
    "        ) * (1) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 2]),\n",
       " torch.Size([8, 2]),\n",
       " tensor([[0.6097, 0.3903],\n",
       "         [0.5698, 0.4302],\n",
       "         [0.5829, 0.4171],\n",
       "         [0.5860, 0.4140],\n",
       "         [0.5920, 0.4080],\n",
       "         [0.5839, 0.4161],\n",
       "         [0.5857, 0.4143],\n",
       "         [0.5875, 0.4125]], grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 1\n",
    "logits_gate = hardness_gate(pooled_output)\n",
    "gate_weights = F.softmax(logits_gate/T, dim=1)\n",
    "logits_gate.shape, gate_weights.shape, gate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 只使用 `easy expert` 进行训练和预测\n",
    "weight rank:\n",
    "- ambi-easy\n",
    "- easy\n",
    "- ambi-hard\n",
    "- hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6097, 0.5698, 0.5829, 0.5860, 0.5920, 0.5839, 0.5857, 0.5875],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_probs = gate_weights[:,0]\n",
    "easy_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.8903, 0.9302, 0.9171, 0.9140, 0.9080, 0.9161, 0.9143, 0.9125],\n",
       "        grad_fn=<SWhereBackward>),\n",
       " tensor([0.9753, 1.0191, 1.0047, 1.0013, 0.9947, 1.0036, 1.0017, 0.9997],\n",
       "        grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_weights = torch.where(easy_probs>0.5, 1-torch.abs(easy_probs-0.5), easy_probs)\n",
    "# 归一化\n",
    "batch_size = 8\n",
    "easy_weights, easy_weights * batch_size / torch.sum(easy_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4060, 0.3822, 0.9169, 0.4725, 1.1667, 0.2972, 0.4749, 0.9775],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor(0.6367, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_weights = easy_weights * batch_size / torch.sum(easy_weights)\n",
    "example_loss = torch.tensor([0.4163, 0.3751, 0.9126, 0.4719, 1.1729, 0.2961, 0.4741, 0.9778])\n",
    "easy_weights * example_loss, torch.mean(easy_weights * example_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of 只使用 `easy expert` 进行训练和预测.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7000, 0.3000],\n",
       "         [0.4000, 0.6000]]),\n",
       " tensor([[0.8000, 0.3000],\n",
       "         [0.4000, 0.9000]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.7,0.3],[0.4,0.6]])\n",
    "X, torch.where(X>0.5, 1-torch.abs(X-0.5), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output = dropout(pooled_output)\n",
    "logits_easy = classifier_easy(pooled_output)\n",
    "logits_hard = classifier_easy(pooled_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000]),\n",
       " tensor([[0.7000],\n",
       "         [0.7000],\n",
       "         [0.7000],\n",
       "         [0.7000],\n",
       "         [0.7000],\n",
       "         [0.7000],\n",
       "         [0.7000],\n",
       "         [0.7000]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3000],\n",
       "         [0.3000],\n",
       "         [0.3000],\n",
       "         [0.3000],\n",
       "         [0.3000],\n",
       "         [0.3000],\n",
       "         [0.3000]]),\n",
       " tensor([[0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidences = torch.tensor([0.7]*8)\n",
    "confidences\n",
    "easy_probs = confidences.view(-1,1)\n",
    "hard_probs = 1 - easy_probs\n",
    "hardness_probs = torch.cat([easy_probs,hard_probs],dim=1)\n",
    "confidences, easy_probs, hard_probs, hardness_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6693, -0.7176],\n",
       "         [-0.6422, -0.7469],\n",
       "         [-0.6316, -0.7587],\n",
       "         [-0.6843, -0.7021],\n",
       "         [-0.6417, -0.7474],\n",
       "         [-0.6898, -0.6965],\n",
       "         [-0.6593, -0.7282],\n",
       "         [-0.6960, -0.6903]], grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000],\n",
       "         [0.7000, 0.3000]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(logits_gate, dim=-1),hardness_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0712, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_gate =  F.kl_div(F.log_softmax(logits_gate, dim=-1), hardness_probs, reduction='batchmean')\n",
    "loss_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4163, 0.3751, 0.9126, 0.4719, 1.1729, 0.2961, 0.4741, 0.9778],\n",
       "        grad_fn=<NllLossBackward>),\n",
       " tensor([0.4163, 0.3751, 0.9126, 0.4719, 1.1729, 0.2961, 0.4741, 0.9778],\n",
       "        grad_fn=<NllLossBackward>))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = config.num_labels\n",
    "labels = batch['labels']\n",
    "loss_fct = CrossEntropyLoss(reduction='none')\n",
    "loss_easy = loss_fct(logits_easy.view(-1, num_labels), labels.view(-1))\n",
    "loss_hard = loss_fct(logits_hard.view(-1, num_labels), labels.view(-1))\n",
    "loss_easy, loss_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4163, 0.4163],\n",
       "        [0.3751, 0.3751],\n",
       "        [0.9126, 0.9126],\n",
       "        [0.4719, 0.4719],\n",
       "        [1.1729, 1.1729],\n",
       "        [0.2961, 0.2961],\n",
       "        [0.4741, 0.4741],\n",
       "        [0.9778, 0.9778]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_hard_loss_cat = torch.cat([loss_easy.view(-1,1), loss_hard.view(-1,1)],dim=1)\n",
    "easy_hard_loss_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4163, 0.4163],\n",
       "         [0.3751, 0.3751],\n",
       "         [0.9126, 0.9126],\n",
       "         [0.4719, 0.4719],\n",
       "         [1.1729, 1.1729],\n",
       "         [0.2961, 0.2961],\n",
       "         [0.4741, 0.4741],\n",
       "         [0.9778, 0.9778]], grad_fn=<CatBackward>),\n",
       " tensor([[0.5121, 0.4879],\n",
       "         [0.5262, 0.4738],\n",
       "         [0.5317, 0.4683],\n",
       "         [0.5044, 0.4956],\n",
       "         [0.5264, 0.4736],\n",
       "         [0.5017, 0.4983],\n",
       "         [0.5172, 0.4828],\n",
       "         [0.4986, 0.5014]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.2132, 0.2031],\n",
       "         [0.1974, 0.1777],\n",
       "         [0.4853, 0.4274],\n",
       "         [0.2380, 0.2338],\n",
       "         [0.6174, 0.5555],\n",
       "         [0.1486, 0.1476],\n",
       "         [0.2452, 0.2289],\n",
       "         [0.4875, 0.4903]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_hard_loss_cat, gate_weights, easy_hard_loss_cat * gate_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3185, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_loss = easy_hard_loss_cat * gate_weights\n",
    "torch.mean(weighted_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class HCTForSequenceClassification(nn.Module):\n",
    "    def __init__(self, model_name_or_path, config):\n",
    "        super(HCTForSequenceClassification, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name_or_path)\n",
    "        self.config = config\n",
    "        self.num_labels = config.num_labels\n",
    "        self.classifier_dropout = (\n",
    "        self.config.classifier_dropout if self.config.classifier_dropout is not None else self.config.hidden_dropout_prob\n",
    "    )\n",
    "        self.dropout = nn.Dropout(self.classifier_dropout)\n",
    "        self.classifier_easy = nn.Linear(self.config.hidden_size, config.num_labels)\n",
    "        self.classifier_hard = nn.Linear(self.config.hidden_size, config.num_labels)\n",
    "        # 2 experts: easy or hard\n",
    "        # gate output: 0 for easy, 1 for hard\n",
    "        self.hardness_gate = nn.Linear(self.config.hidden_size,2) \n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        confidences: Optional[torch.Tensor] = None,  # the confidence value of a sample\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # the CLS vector\n",
    "        pooled_output = outputs[1]  \n",
    "        # gating:\n",
    "        logits_gate = self.hardness_gate(pooled_output)\n",
    "        gate_weights = F.softmax(logits_gate)\n",
    "        # easy/hard experts:\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits_easy = self.classifier_easy(pooled_output)\n",
    "        logits_hard = self.classifier_hard(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        loss_easy, loss_hard, gate_loss = None, None, None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            # gating loss\n",
    "            # confidences 其实相当于 easy 分支的概率，所以你还需要自己构造一个 hard prob\n",
    "            easy_probs = confidences.view(-1,1)\n",
    "            hard_probs = 1 - easy_probs\n",
    "            hardness_probs = torch.cat([easy_probs,hard_probs],dim=1)\n",
    "            gate_loss = F.kl_div(F.log_softmax(logits_gate, dim=-1), hardness_probs, reduction='batchmean')\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss(reduction='none')\n",
    "                if self.num_labels == 1:\n",
    "                    loss_easy = loss_fct(logits_easy.squeeze(), labels.squeeze())\n",
    "                    loss_hard = loss_fct(logits_hard.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss_easy = loss_fct(logits_easy, labels)\n",
    "                    loss_hard = loss_fct(logits_hard, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss(reduction='none') # reduction='none', 来得到每个sample的loss\n",
    "                loss_easy = loss_fct(logits_easy.view(-1, self.num_labels), labels.view(-1))\n",
    "                loss_hard = loss_fct(logits_hard.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss(reduction='none')\n",
    "                loss_easy = loss_fct(logits_easy, labels)\n",
    "                loss_hard = loss_fct(logits_hard, labels)\n",
    "            \n",
    "            easy_hard_loss_cat = torch.cat([loss_easy.view(-1,1), loss_hard.view(-1,1)],dim=1)\n",
    "            weighted_loss = easy_hard_loss_cat * gate_weights\n",
    "            clf_loss = torch.mean(weighted_loss)\n",
    "            loss = gate_loss + clf_loss\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (logits_gate,logits_easy, logits_hard,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits={\"gate\":logits_gate, \"easy\":logits_easy, \"hard\":logits_hard},\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "my_model = HCTForSequenceClassification('bert-base-cased', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.5328, grad_fn=<AddBackward0>),\n",
       " {'gate': tensor([[0.4412, 0.2590],\n",
       "          [0.4171, 0.3171],\n",
       "          [0.4294, 0.3268],\n",
       "          [0.4348, 0.3207],\n",
       "          [0.3658, 0.3255],\n",
       "          [0.4153, 0.2928],\n",
       "          [0.3959, 0.2656],\n",
       "          [0.4340, 0.2752]], grad_fn=<AddmmBackward>),\n",
       "  'easy': tensor([[ 0.0633,  0.9459],\n",
       "          [-0.1190,  0.8201],\n",
       "          [ 0.1919,  0.9389],\n",
       "          [-0.2952,  0.6612],\n",
       "          [ 0.0297,  0.8347],\n",
       "          [-0.2183,  1.2751],\n",
       "          [-0.1609,  1.0679],\n",
       "          [-0.1628,  0.9590]], grad_fn=<AddmmBackward>),\n",
       "  'hard': tensor([[-0.2337,  0.2637],\n",
       "          [-0.3422,  0.3396],\n",
       "          [-0.2908,  0.3610],\n",
       "          [-0.2687,  0.1167],\n",
       "          [-0.1619, -0.1485],\n",
       "          [-0.7175,  0.3527],\n",
       "          [-0.4886,  0.2593],\n",
       "          [-0.1387,  0.3856]], grad_fn=<AddmmBackward>)})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_outputs = my_model(**batch, confidences=confidences)\n",
    "my_outputs.loss, my_outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices = torch.argmax(my_outputs.logits['gate'],dim=1)\n",
    "indices = torch.tensor([1,1,1,1,0,0,0,0])\n",
    "indices, 1-indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2337,  0.2637],\n",
       "        [-0.3422,  0.3396],\n",
       "        [-0.2908,  0.3610],\n",
       "        [-0.2687,  0.1167],\n",
       "        [ 0.0297,  0.8347],\n",
       "        [-0.2183,  1.2751],\n",
       "        [-0.1609,  1.0679],\n",
       "        [-0.1628,  0.9590]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_outputs.logits['easy'] * (1-indices).view(-1,1), my_outputs.logits['hard'] * indices.view(-1,1)\n",
    "\n",
    "my_outputs.logits['easy'] * (1-indices).view(-1,1) + my_outputs.logits['hard'] * indices.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0633,  0.9459],\n",
       "         [-0.1190,  0.8201],\n",
       "         [ 0.1919,  0.9389],\n",
       "         [-0.2952,  0.6612],\n",
       "         [ 0.0297,  0.8347],\n",
       "         [-0.2183,  1.2751],\n",
       "         [-0.1609,  1.0679],\n",
       "         [-0.1628,  0.9590]], grad_fn=<AddmmBackward>),\n",
       " tensor([[-0.2337,  0.2637],\n",
       "         [-0.3422,  0.3396],\n",
       "         [-0.2908,  0.3610],\n",
       "         [-0.2687,  0.1167],\n",
       "         [-0.1619, -0.1485],\n",
       "         [-0.7175,  0.3527],\n",
       "         [-0.4886,  0.2593],\n",
       "         [-0.1387,  0.3856]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_outputs.logits['easy'], my_outputs.logits['hard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5454, 0.4546],\n",
       "         [0.5250, 0.4750],\n",
       "         [0.5256, 0.4744],\n",
       "         [0.5285, 0.4715],\n",
       "         [0.5101, 0.4899],\n",
       "         [0.5306, 0.4694],\n",
       "         [0.5325, 0.4675],\n",
       "         [0.5396, 0.4604]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.5454, 0.5250, 0.5256, 0.5285, 0.5101, 0.5306, 0.5325, 0.5396],\n",
       "        grad_fn=<SelectBackward>),\n",
       " tensor([0.4546, 0.4750, 0.4744, 0.4715, 0.4899, 0.4694, 0.4675, 0.4604],\n",
       "        grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(my_outputs.logits['gate'], dim=1)\n",
    "weights, weights[:,0], weights[:,1]  # 第0列是easy的权重，第1列是hard的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0345,  0.5159],\n",
       "         [-0.0625,  0.4305],\n",
       "         [ 0.1009,  0.4935],\n",
       "         [-0.1560,  0.3494],\n",
       "         [ 0.0152,  0.4258],\n",
       "         [-0.1158,  0.6765],\n",
       "         [-0.0857,  0.5687],\n",
       "         [-0.0879,  0.5175]], grad_fn=<MulBackward0>),\n",
       " tensor([[-0.1062,  0.1199],\n",
       "         [-0.1626,  0.1613],\n",
       "         [-0.1380,  0.1712],\n",
       "         [-0.1267,  0.0550],\n",
       "         [-0.0793, -0.0728],\n",
       "         [-0.3368,  0.1656],\n",
       "         [-0.2284,  0.1212],\n",
       "         [-0.0639,  0.1775]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_outputs.logits['easy'] * weights[:,0].view(-1,1), my_outputs.logits['hard'] * weights[:,1].view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/v-biyangguo/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fafeef372dd4a3d99f7233786ee4508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 550152\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "snli_data = load_dataset('snli')\n",
    "snli_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 3219, 0: 3368, 2: 3237, -1: 176})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(snli_data['test']['label'])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/v-biyangguo/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-bd229a5e884be60a.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label'],\n",
       "    num_rows: 549367\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_data['train'] = snli_data['train'].filter(lambda x:x['label']!=-1)\n",
    "snli_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "550152-785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('conda': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}